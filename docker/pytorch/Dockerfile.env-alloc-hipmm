# Ablation: PYTORCH_HIP_ALLOC_CONF with hipMallocManaged backend
# Tests managed memory instead of native allocator

ARG FEDORA_VERSION=43
FROM fedora:${FEDORA_VERSION}

LABEL maintainer="softab"
LABEL description="PyTorch with hipMallocManaged allocator"
LABEL ablation.variable="PYTORCH_HIP_ALLOC_CONF"
LABEL ablation.value="backend:hipmm"

ARG GFX_TARGET=gfx1151

ENV LANG=C.UTF-8
ENV LC_ALL=C.UTF-8

RUN dnf -y update && dnf -y install \
    python3.12 python3.12-devel \
    rocm-hip-devel rocblas-devel hipblas-devel hipblaslt-devel \
    rocminfo rocm-smi hipcc \
    gcc gcc-c++ cmake ninja-build bc jq htop wget \
    && dnf clean all && python3.12 -m ensurepip --upgrade

RUN python3.12 -m pip install --upgrade pip numpy && \
    python3.12 -m pip install --index-url https://download.pytorch.org/whl/rocm6.2 \
    torch torchvision torchaudio

ENV HSA_ENABLE_SDMA=0
ENV ROCBLAS_USE_HIPBLASLT=1
# KEY DIFFERENCE: hipMallocManaged backend instead of native
ENV PYTORCH_HIP_ALLOC_CONF="backend:hipmm,expandable_segments:True"
ENV TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1
ENV HIP_VISIBLE_DEVICES=0
ENV HSA_OVERRIDE_GFX_VERSION=11.0.0

WORKDIR /workspace

COPY <<'EOF' /usr/local/bin/bench-pytorch
#!/usr/bin/env python3.12
import torch, time, json, os

results = {
    "pytorch_version": torch.__version__,
    "rocm_version": str(torch.version.hip),
    "ablation_var": "PYTORCH_HIP_ALLOC_CONF",
    "ablation_val": os.environ.get("PYTORCH_HIP_ALLOC_CONF", "?"),
    "device": None, "status": "unknown", "benchmarks": {}
}

if not torch.cuda.is_available():
    results["status"] = "FAIL"
    results["error"] = "CUDA not available"
    print(json.dumps(results, indent=2)); exit(1)

results["device"] = torch.cuda.get_device_name(0)
try:
    x = torch.randn(2048, 2048, device='cuda', dtype=torch.float16)
    for _ in range(3): _ = torch.mm(x, x)
    torch.cuda.synchronize()

    for size in [1024, 2048, 4096, 8192]:
        a = torch.randn(size, size, device='cuda', dtype=torch.float16)
        b = torch.randn(size, size, device='cuda', dtype=torch.float16)
        torch.cuda.synchronize()
        start = time.perf_counter()
        for _ in range(10): c = torch.mm(a, b)
        torch.cuda.synchronize()
        elapsed = (time.perf_counter() - start) / 10
        tflops = (2 * size**3 / elapsed) / 1e12
        results["benchmarks"][f"matmul_{size}"] = {"tflops": round(tflops, 2), "time_ms": round(elapsed*1000, 3)}

    results["peak_tflops"] = max(v["tflops"] for v in results["benchmarks"].values())
    results["status"] = "SUCCESS"
except Exception as e:
    results["status"] = "FAIL"; results["error"] = str(e)

print(json.dumps(results, indent=2))
EOF
RUN chmod +x /usr/local/bin/bench-pytorch

CMD ["bench-pytorch"]
