# WhisperX with ROCm 7.2 - Combined Transcription + Speaker Diarization
# Combines faster-whisper (transcription) + pyannote-audio (diarization)
# Provides word-level timestamps with speaker labels
#
# Run: podman run --rm --device=/dev/kfd --device=/dev/dri --ipc=host \
#        -e HF_TOKEN=your_token_here \
#        -v ~/samples:/samples:ro \
#        softab:whisperx-rocm72-gfx1151 \
#        whisperx /samples/audio.wav --model base.en --diarize --language en

FROM docker.io/rocm/dev-ubuntu-24.04:7.2-complete

LABEL maintainer="softab"
LABEL description="WhisperX (Faster-Whisper + pyannote diarization) ROCm 7.2 for Strix Halo"
LABEL backend="pytorch-rocm"
LABEL rocm.version="7.2"
LABEL python.version="3.11"
LABEL features="transcription,diarization"

ARG GFX_TARGET=gfx1151

ENV LANG=C.UTF-8
ENV LC_ALL=C.UTF-8
ENV DEBIAN_FRONTEND=noninteractive

# Install Python 3.11, ffmpeg, and build tools
RUN apt-get update && apt-get install -y --no-install-recommends \
    software-properties-common \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update && apt-get install -y --no-install-recommends \
    python3.11 \
    python3.11-dev \
    python3.11-venv \
    python3-pip \
    ffmpeg \
    git \
    wget \
    bc jq htop \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 && \
    update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1

# ROCm environment
ENV PATH="/opt/rocm/bin:${PATH}"
ENV LD_LIBRARY_PATH="/opt/rocm/lib:${LD_LIBRARY_PATH}"
ENV ROCM_PATH="/opt/rocm"
ENV HIP_PATH="/opt/rocm"
ENV CMAKE_PREFIX_PATH="/opt/rocm"

# Strix Halo settings
ENV HSA_ENABLE_SDMA=0
ENV HIP_VISIBLE_DEVICES=0
ENV GPU_TARGETS=${GFX_TARGET}
ENV AMDGPU_TARGETS=${GFX_TARGET}

# PyTorch environment
ENV PYTORCH_ROCM_ARCH="gfx1151"
ENV ROCBLAS_USE_HIPBLASLT=1

# Install PyTorch for ROCm 6.2
RUN python3 -m pip install --no-cache-dir \
    torch torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/rocm6.2

# Install WhisperX and dependencies
RUN python3 -m pip install --no-cache-dir \
    git+https://github.com/m-bain/whisperx.git \
    pyannote.audio \
    librosa \
    soundfile \
    numpy

WORKDIR /workspace

# Create test script
COPY <<'EOF' /workspace/test_whisperx.py
#!/usr/bin/env python3
"""Test script for WhisperX on ROCm"""
import os
import sys
import torch
import whisperx

def main():
    print("=== WhisperX ROCm 7.2 Test ===")
    print(f"Python: {sys.version}")
    print(f"PyTorch: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        print(f"CUDA device: {torch.cuda.get_device_name(0)}")
        print(f"CUDA device count: {torch.cuda.device_count()}")

    hf_token = os.environ.get('HF_TOKEN')
    if not hf_token:
        print("\nWARNING: HF_TOKEN not set")
        print("  - Transcription will work")
        print("  - Diarization will fail (requires token)")
        print("Get token from: https://huggingface.co/settings/tokens")

    print("\nWhisperX combines:")
    print("  1. Faster-Whisper (transcription with word-level timestamps)")
    print("  2. pyannote-audio (speaker diarization)")
    print("  3. Phoneme alignment for accuracy")

    print("\nTest completed successfully!")
    print("WhisperX is ready for use")
    print("\nUsage:")
    print("  whisperx audio.wav --model base.en --diarize --language en")
    return 0

if __name__ == "__main__":
    sys.exit(main())
EOF

RUN chmod +x /workspace/test_whisperx.py

# Create benchmark script
COPY <<'EOF' /workspace/bench_whisperx.sh
#!/bin/bash
# Benchmark WhisperX with diarization

AUDIO="${1:-/samples/audio.wav}"
MODEL="${2:-base.en}"

if [ ! -f "$AUDIO" ]; then
    echo "Usage: bash bench_whisperx.sh <audio_file> [model]"
    echo "Example: bash bench_whisperx.sh /samples/audio.wav base.en"
    exit 1
fi

if [ -z "$HF_TOKEN" ]; then
    echo "ERROR: HF_TOKEN environment variable required for diarization"
    exit 1
fi

echo "=== WhisperX Benchmark ==="
echo "Audio: $AUDIO"
echo "Model: $MODEL"
echo "Device: $(python3 -c 'import torch; print("CUDA" if torch.cuda.is_available() else "CPU")')"
echo ""

# Run WhisperX with diarization
time whisperx "$AUDIO" \
    --model "$MODEL" \
    --diarize \
    --highlight_words True \
    --max_line_width 80 \
    --max_line_count 1 \
    --output_dir /workspace/output \
    --output_format all

echo ""
echo "=== Output Files ==="
ls -lh /workspace/output/

echo ""
echo "=== Results Preview ==="
if [ -f /workspace/output/*.json ]; then
    head -50 /workspace/output/*.json
fi
EOF

RUN chmod +x /workspace/bench_whisperx.sh

# Create README
COPY <<'EOF' /workspace/README_WHISPERX.md
# WhisperX with ROCm on Strix Halo

WhisperX provides:
- Accurate transcription (Faster-Whisper)
- Word-level timestamps (phoneme alignment)
- Speaker diarization (pyannote-audio)
- GPU acceleration via ROCm

## Quick Start

### Transcription Only (No Token Required)
```bash
whisperx audio.wav --model base.en --language en
```

### Transcription + Diarization (Requires HF Token)
```bash
export HF_TOKEN=hf_your_token_here
whisperx audio.wav --model base.en --diarize --language en
```

## Output Formats

WhisperX can output:
- JSON (with word-level timestamps and speaker labels)
- SRT (subtitles)
- VTT (web subtitles)
- TXT (plain text)
- TSV (tab-separated)

Example:
```bash
whisperx audio.wav --model base.en --diarize \
  --output_format json,srt,txt --output_dir ./output
```

## GPU Performance

WhisperX uses GPU for:
1. Whisper transcription (~10-50x faster than CPU)
2. pyannote speaker embeddings (~10-50x faster than CPU)
3. Alignment model (moderate speedup)

## Available Models

Whisper models (quality vs speed tradeoff):
- tiny.en, tiny - Fastest, lowest quality
- base.en, base - Good balance
- small.en, small - Better quality
- medium.en, medium - High quality
- large-v2, large-v3 - Best quality, slowest

## Requirements

- FFmpeg (for audio processing)
- HuggingFace token (for diarization only)
- Accept model conditions at:
  - https://huggingface.co/pyannote/speaker-diarization-community-1
  - https://huggingface.co/pyannote/segmentation-3.0

## Example Output

```json
{
  "segments": [
    {
      "start": 0.5,
      "end": 3.2,
      "text": "Hello, how are you today?",
      "speaker": "SPEAKER_00",
      "words": [
        {"word": "Hello", "start": 0.5, "end": 0.8, "score": 0.99},
        {"word": "how", "start": 0.9, "end": 1.1, "score": 0.98},
        ...
      ]
    },
    {
      "start": 3.5,
      "end": 6.1,
      "text": "I'm doing great, thanks!",
      "speaker": "SPEAKER_01",
      ...
    }
  ]
}
```
EOF

LABEL gfx.target="${GFX_TARGET}"
LABEL ablation.type="whisperx"
LABEL ablation.backend="pytorch-rocm72"
LABEL ablation.features="transcription,diarization,alignment"

CMD ["python3", "test_whisperx.py"]
