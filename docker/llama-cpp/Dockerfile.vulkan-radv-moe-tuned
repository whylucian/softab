# llama.cpp optimized for MoE (Mixture of Experts) models
# From Reddit: Strix Halo excels at MoE due to large VRAM + decent compute
# MoE models have smaller active parameters, so batch size and FA matter more
#
# Recommended models: Qwen3-30B-A3B, dots1, Llama 4 Scout, Hunyuan
#
# Build: podman build -t softab:llama-moe -f Dockerfile.moe-optimized .
# Run:   podman run --device /dev/dri -v ~/models:/models -e MODEL=/models/qwen3-30b-a3b.gguf softab:llama-moe

FROM fedora:43

LABEL maintainer="softab"
LABEL description="llama.cpp Vulkan optimized for MoE models"
LABEL ablation.type="llama-cpp"
LABEL ablation.backend="vulkan-moe"
LABEL ablation.note="Optimized for MoE: FA enabled, batch size 256, Vulkan RADV"

ENV LANG=C.UTF-8
ENV LC_ALL=C.UTF-8

# Install dependencies + Mesa RADV (best stability for MoE)
RUN dnf -y update && dnf -y install \
    gcc gcc-c++ cmake ninja-build git \
    python3 python3-pip \
    vulkan-loader vulkan-loader-devel vulkan-headers vulkan-tools \
    mesa-vulkan-drivers \
    glslc glslang \
    bc jq htop \
    && dnf clean all

# Force RADV (more stable than AMDVLK for long MoE runs)
ENV AMD_VULKAN_ICD=RADV
ENV GGML_VK_PREFER_HOST_MEMORY=ON

# Clone and build llama.cpp
RUN git clone --depth 1 https://github.com/ggerganov/llama.cpp /llama.cpp

WORKDIR /llama.cpp

RUN cmake -B build-vulkan \
    -DGGML_VULKAN=ON \
    -DCMAKE_BUILD_TYPE=Release \
    -G Ninja && \
    cmake --build build-vulkan -j$(nproc)

RUN ln -sf /llama.cpp/build-vulkan/bin/llama-cli /usr/local/bin/llama-cli && \
    ln -sf /llama.cpp/build-vulkan/bin/llama-bench /usr/local/bin/llama-bench && \
    ln -sf /llama.cpp/build-vulkan/bin/llama-server /usr/local/bin/llama-server

WORKDIR /workspace

# Benchmark script optimized for MoE
COPY <<'EOF' /usr/local/bin/bench-llama
#!/bin/bash
echo "=== llama.cpp MoE-Optimized Benchmark ==="
echo "AMD_VULKAN_ICD: $AMD_VULKAN_ICD"
echo "Configuration: FA enabled, batch size 256"
echo ""
echo "Recommended MoE models for Strix Halo 128GB:"
echo "  - Qwen3-30B-A3B (3B active): ~72 t/s generation"
echo "  - dots1 142B (14B active): ~21 t/s generation"
echo "  - Llama 4 Scout 109B (17B active): ~17-19 t/s generation"
echo "  - Hunyuan 80B (13B active): ~17 t/s generation"
echo ""
echo "Vulkan device:"
vulkaninfo 2>/dev/null | grep -E "(deviceName|driverName)" | head -4
echo ""

if [ -z "$MODEL" ]; then
    echo "Set MODEL=/path/to/model.gguf"
    exit 1
fi

echo "Model: $MODEL"
echo ""

# MoE-optimized settings:
# -fa 1: Flash Attention (critical for MoE)
# -b 256: Batch size 256 (Reddit shows this optimal for many MoE models)
# --mmap 0: Disable mmap (required for Strix Halo)
# -ngl 999: All layers on GPU
llama-bench -m "$MODEL" \
    --mmap 0 \
    -ngl 999 \
    -fa 1 \
    -b 256 \
    -p 512,2048,8192 \
    -n 128
EOF
RUN chmod +x /usr/local/bin/bench-llama

# Interactive chat optimized for MoE
COPY <<'EOF' /usr/local/bin/chat-moe
#!/bin/bash
if [ -z "$MODEL" ]; then
    echo "Set MODEL=/path/to/model.gguf"
    exit 1
fi

echo "Starting MoE-optimized chat..."
echo "Model: $MODEL"
echo ""

llama-cli -m "$MODEL" \
    --mmap 0 \
    -ngl 999 \
    -fa 1 \
    -b 256 \
    --ctx-size 8192 \
    -i
EOF
RUN chmod +x /usr/local/bin/chat-moe

CMD ["bench-llama"]
