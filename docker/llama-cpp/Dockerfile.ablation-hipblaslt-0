# llama.cpp HIP ablation: hipBLASLt DISABLED
# Compare with Dockerfile.ablation-hipblaslt-1 to measure hipBLASLt impact
# Reddit claims "ROCBLAS_USE_HIPBLASLT=1 almost always faster than default rocBLAS"
#
# Build: podman build -t softab:llama-hipblaslt-0 -f Dockerfile.ablation-hipblaslt-0 .
# Run:   podman run --device /dev/kfd --device /dev/dri -v ~/models:/models -e MODEL=/models/test.gguf softab:llama-hipblaslt-0

FROM docker.io/rocm/dev-ubuntu-24.04:7.2-complete

LABEL maintainer="softab"
LABEL description="llama.cpp HIP with hipBLASLt DISABLED (ablation baseline)"
LABEL ablation.type="llama-cpp"
LABEL ablation.variable="ROCBLAS_USE_HIPBLASLT"
LABEL ablation.value="0"
LABEL ablation.note="Baseline: rocBLAS without hipBLASLt"

ARG GFX_TARGET=gfx1151

ENV LANG=C.UTF-8
ENV LC_ALL=C.UTF-8

# Install build tools
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential cmake ninja-build git \
    bc jq htop \
    && rm -rf /var/lib/apt/lists/*

# Environment
ENV PATH="/opt/rocm/bin:${PATH}"
ENV LD_LIBRARY_PATH="/opt/rocm/lib:${LD_LIBRARY_PATH}"
ENV ROCM_PATH="/opt/rocm"
ENV HIP_PATH="/opt/rocm"
ENV CMAKE_PREFIX_PATH="/opt/rocm"

# Strix Halo settings - hipBLASLt DISABLED
ENV HSA_ENABLE_SDMA=0
ENV ROCBLAS_USE_HIPBLASLT=0
ENV HIP_VISIBLE_DEVICES=0

# GFX target
ENV GPU_TARGETS=${GFX_TARGET}
ENV AMDGPU_TARGETS=${GFX_TARGET}

# Clone llama.cpp
RUN git clone --depth 1 https://github.com/ggerganov/llama.cpp /llama.cpp

WORKDIR /llama.cpp

# Build with HIP
RUN HIPCXX="$(hipconfig -l)/clang" HIP_PATH="$(hipconfig -R)" \
    cmake -B build-hip \
    -DGGML_HIP=ON \
    -DAMDGPU_TARGETS="${GFX_TARGET}" \
    -DGGML_HIP_UMA=OFF \
    -DCMAKE_BUILD_TYPE=Release \
    -G Ninja && \
    cmake --build build-hip --parallel $(nproc)

# Create symlinks
RUN ln -sf /llama.cpp/build-hip/bin/llama-cli /usr/local/bin/llama-cli && \
    ln -sf /llama.cpp/build-hip/bin/llama-bench /usr/local/bin/llama-bench

WORKDIR /workspace

# Benchmark script
COPY <<'EOF' /usr/local/bin/bench-llama
#!/bin/bash
echo "=== llama.cpp hipBLASLt Ablation: DISABLED ==="
echo "ROCBLAS_USE_HIPBLASLT: $ROCBLAS_USE_HIPBLASLT"
echo "GFX Target: ${GPU_TARGETS}"
echo "ROCm: $(cat /opt/rocm/.info/version 2>/dev/null || echo unknown)"
echo ""
rocminfo 2>/dev/null | grep -A1 "Marketing Name" | head -2
echo ""

if [ -z "$MODEL" ]; then
    echo "Set MODEL=/path/to/model.gguf"
    exit 1
fi

echo "Model: $MODEL"
echo ""

llama-bench -m "$MODEL" --mmap 0 -ngl 999 -fa 1 -p 512 -n 128
EOF
RUN chmod +x /usr/local/bin/bench-llama

LABEL gfx.target="${GFX_TARGET}"
LABEL ablation.backend="hip-hipblaslt-0"

CMD ["bench-llama"]
